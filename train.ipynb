{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['latitude', 'longitude', 'altitude', 'rainfall_intensity',\n",
      "       'temperature', 'humidity', 'atmospheric_pressure', 'river_level',\n",
      "       'drainage_capacity', 'drainage_system_condition', 'population_density',\n",
      "       'urbanization_level', 'flood'],\n",
      "      dtype='object')\n",
      "Index(['objectid', 'ward_name', 'wardno', 'locationname', 'kgisfvlid', 'zone',\n",
      "       'longitude', 'latitude'],\n",
      "      dtype='object')\n",
      "Index(['date', 'latitude', 'longitude', 'temperature_2m',\n",
      "       'relative_humidity_2m', 'precipitation', 'rain', 'weather_code'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "blore_urban_flood = pd.read_csv('bangalore_urban_flood_prediction_AI.csv')\n",
    "print(blore_urban_flood.columns)\n",
    "\n",
    "flood_vulnerable_loc = pd.read_csv(\"flood_vulnerable_locations.csv\")\n",
    "print(flood_vulnerable_loc.columns)\n",
    "\n",
    "weather_data = pd.read_csv(\"weather_data_all_locations.csv\")\n",
    "print(weather_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    latitude  longitude  altitude  rainfall_intensity  temperature  humidity  \\\n",
      "0  13.031867  77.531658    997.63               40.06        33.83     98.73   \n",
      "1  12.877241  77.525416    893.85              138.76        29.91     83.63   \n",
      "2  12.899599  77.517722    934.88               65.84        16.35     73.36   \n",
      "3  12.893955  77.552339    929.47              138.25        31.94     44.72   \n",
      "4  12.984063  77.538374    901.12              125.92        33.56     47.63   \n",
      "\n",
      "   atmospheric_pressure  river_level  drainage_capacity  \\\n",
      "0                931.65         4.58              89.96   \n",
      "1                922.12         6.65               1.30   \n",
      "2                934.76         5.55              76.12   \n",
      "3                930.82         6.46              22.18   \n",
      "4                941.29         6.83              30.38   \n",
      "\n",
      "   drainage_system_condition  population_density  urbanization_level  flood  \n",
      "0                          5            24933.97                   3      1  \n",
      "1                          4             9937.05                  10      1  \n",
      "2                          9            11088.95                   2      0  \n",
      "3                          4            12124.49                   7      0  \n",
      "4                          5             8446.10                   5      1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier  # You can use other classifiers like LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load your blore_urban_flood dataset (replace with your actual file path)\n",
    "blore_urban_flood = pd.read_csv('bangalore_urban_flood_prediction_AI.csv')\n",
    "\n",
    "# Display the first few rows to understand the structure of the data\n",
    "print(blore_urban_flood.head())\n",
    "\n",
    "# Prepare the features (X) and target (y)\n",
    "X = blore_urban_flood[['temperature', 'humidity',  'river_level']]  # Weather features\n",
    "y = blore_urban_flood['flood']  # Target variable (flood = 0 or 1)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5083333333333333\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51       301\n",
      "           1       0.51      0.51      0.51       299\n",
      "\n",
      "    accuracy                           0.51       600\n",
      "   macro avg       0.51      0.51      0.51       600\n",
      "weighted avg       0.51      0.51      0.51       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model (you can also use Logistic Regression or any other classifier)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the accuracy on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Print the accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        date   latitude  longitude  temperature   humidity  \\\n",
      "0  2022-01-01 00:00:00+00:00  12.867499  77.532435    19.066000  87.912346   \n",
      "1  2022-01-01 01:00:00+00:00  12.867499  77.532435    19.116001  88.474860   \n",
      "2  2022-01-01 02:00:00+00:00  12.867499  77.532435    19.416000  88.499740   \n",
      "3  2022-01-01 03:00:00+00:00  12.867499  77.532435    20.466002  82.145355   \n",
      "4  2022-01-01 04:00:00+00:00  12.867499  77.532435    22.166000  71.256330   \n",
      "\n",
      "   precipitation  river_level  weather_code  rainfall_intensity  \\\n",
      "0            0.0          0.0           3.0                   0   \n",
      "1            0.0          0.0           3.0                   0   \n",
      "2            0.1          0.1          51.0                   0   \n",
      "3            0.0          0.0           3.0                   0   \n",
      "4            0.0          0.0           3.0                   0   \n",
      "\n",
      "   predicted_flood  \n",
      "0                1  \n",
      "1                1  \n",
      "2                1  \n",
      "3                0  \n",
      "4                0  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "weather_data = pd.read_csv(\"weather_data_all_locations.csv\")\n",
    "\n",
    "# Rename columns to match the ones used for training\n",
    "weather_data.rename(columns={\n",
    "    'temperature_2m': 'temperature',\n",
    "    'relative_humidity_2m': 'humidity',\n",
    "    'rain': 'river_level'\n",
    "}, inplace=True)\n",
    "\n",
    "# Add 'rainfall_intensity' as a column with a default value (e.g., 0)\n",
    "weather_data['rainfall_intensity'] = 0  # You can choose a different default value\n",
    "\n",
    "# Ensure the features are in the same order as during training\n",
    "# Example: Make sure you include all features in the exact order as during training\n",
    "X_weather = weather_data[['temperature', 'humidity',  'river_level']]\n",
    "\n",
    "# Predict the flood occurrences using the trained model\n",
    "predicted_flood = model.predict(X_weather)\n",
    "\n",
    "# Add the predicted flood values to the weather_data DataFrame\n",
    "weather_data['predicted_flood'] = predicted_flood\n",
    "\n",
    "# Display the updated weather data with predictions\n",
    "print(weather_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['temperature', 'humidity', 'river_level']\n"
     ]
    }
   ],
   "source": [
    "# Assuming you trained your model using a DataFrame X_train\n",
    "train_columns_order = X_train.columns.tolist()  # Get the exact order of columns used during training\n",
    "print(train_columns_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns_order = ['temperature', 'humidity', 'rainfall_intensity', 'river_level']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_weather = weather_data[['temperature', 'humidity', 'rainfall_intensity', 'river_level']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        date   latitude  longitude  temperature   humidity  \\\n",
      "0  2022-01-01 00:00:00+00:00  12.867499  77.532435    19.066000  87.912346   \n",
      "1  2022-01-01 01:00:00+00:00  12.867499  77.532435    19.116001  88.474860   \n",
      "2  2022-01-01 02:00:00+00:00  12.867499  77.532435    19.416000  88.499740   \n",
      "3  2022-01-01 03:00:00+00:00  12.867499  77.532435    20.466002  82.145355   \n",
      "4  2022-01-01 04:00:00+00:00  12.867499  77.532435    22.166000  71.256330   \n",
      "5  2022-01-01 05:00:00+00:00  12.867499  77.532435    23.866001  62.875560   \n",
      "6  2022-01-01 06:00:00+00:00  12.867499  77.532435    24.816000  58.086117   \n",
      "7  2022-01-01 07:00:00+00:00  12.867499  77.532435    26.216002  55.006367   \n",
      "8  2022-01-01 08:00:00+00:00  12.867499  77.532435    26.666000  53.056550   \n",
      "9  2022-01-01 09:00:00+00:00  12.867499  77.532435    24.066000  61.337986   \n",
      "\n",
      "   precipitation  river_level  weather_code  predicted_flood  \n",
      "0            0.0          0.0           3.0                1  \n",
      "1            0.0          0.0           3.0                1  \n",
      "2            0.1          0.1          51.0                1  \n",
      "3            0.0          0.0           3.0                0  \n",
      "4            0.0          0.0           3.0                0  \n",
      "5            0.1          0.1          51.0                1  \n",
      "6            0.2          0.2          51.0                1  \n",
      "7            0.2          0.2          51.0                1  \n",
      "8            0.0          0.0           3.0                1  \n",
      "9            0.2          0.2          51.0                1  \n"
     ]
    }
   ],
   "source": [
    "# Load the weather_data (replace with your actual file path)\n",
    "weather_data = pd.read_csv('weather_data_all_locations.csv')\n",
    "\n",
    "# Rename columns to match the ones used for training the model\n",
    "weather_data.rename(columns={\n",
    "    'temperature_2m': 'temperature',\n",
    "    'relative_humidity_2m': 'humidity',\n",
    "    'rain': 'river_level'  # Assuming 'rain' represents the river level in your data\n",
    "}, inplace=True)\n",
    "\n",
    "# Prepare the features for prediction using the correct column names\n",
    "X_weather = weather_data[['temperature', 'humidity', 'river_level']]\n",
    "\n",
    "# Predict the flood occurrences using the trained model\n",
    "predicted_flood = model.predict(X_weather)\n",
    "\n",
    "# Add the predicted flood values to the weather_data DataFrame\n",
    "weather_data['predicted_flood'] = predicted_flood\n",
    "\n",
    "# Display the first 10 rows of the updated weather data with predictions\n",
    "print(weather_data.head(10))\n",
    "\n",
    "# Optionally, save the updated data with predictions to a CSV file\n",
    "weather_data.to_csv('weather_data_with_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Data Columns and First Few Rows:\n",
      "                        date  latitude  longitude  temperature   humidity  \\\n",
      "0  2022-01-01 00:00:00+00:00   12.8675   77.53244    19.066000  87.912346   \n",
      "1  2022-01-01 01:00:00+00:00   12.8675   77.53244    19.116001  88.474860   \n",
      "2  2022-01-01 02:00:00+00:00   12.8675   77.53244    19.416000  88.499740   \n",
      "3  2022-01-01 03:00:00+00:00   12.8675   77.53244    20.466002  82.145355   \n",
      "4  2022-01-01 04:00:00+00:00   12.8675   77.53244    22.166000  71.256330   \n",
      "\n",
      "   precipitation  river_level  weather_code  predicted_flood  \n",
      "0            0.0          0.0           3.0                1  \n",
      "1            0.0          0.0           3.0                1  \n",
      "2            0.1          0.1          51.0                1  \n",
      "3            0.0          0.0           3.0                0  \n",
      "4            0.0          0.0           3.0                0  \n",
      "\n",
      "Flood Vulnerable Locations Columns and First Few Rows:\n",
      "   objectid    ward_name  wardno  \\\n",
      "0         1  Hemmigepura     198   \n",
      "1         2  Hemmigepura     198   \n",
      "2         3  Hemmigepura     198   \n",
      "3         4  Hemmigepura     198   \n",
      "4         5        Begur     192   \n",
      "\n",
      "                                        locationname  kgisfvlid          zone  \\\n",
      "0  Mantri Lake view Appartment Next to Talagattap...          1      RR Nagar   \n",
      "1                                    Kendriya nagara          2      RR Nagar   \n",
      "2                                     Raghuvanahalli          3      RR Nagar   \n",
      "3                     Balaji Layout, Kanakapura Road          4      RR Nagar   \n",
      "4                      Singasandra Country club road          5  Bommanahalli   \n",
      "\n",
      "   longitude   latitude  \n",
      "0  77.532435  12.867499  \n",
      "1  77.528083  12.871775  \n",
      "2  77.542694  12.877711  \n",
      "3  77.541273  12.877865  \n",
      "4  77.637418  12.878907  \n",
      "\n",
      "Merged Data with Location Names and Flood Predictions:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['LocationName', 'WARD_NAME'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Display the merged data to see the location names along with the flood predictions\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMerged Data with Location Names and Flood Predictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmerged_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlongitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatitude\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocationName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWARD_NAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted_flood\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\Neha KB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Neha KB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Neha KB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['LocationName', 'WARD_NAME'] not in index\""
     ]
    }
   ],
   "source": [
    "# Load the flood_vulnerable_loc dataset (replace with your actual file path)\n",
    "flood_vulnerable_loc = pd.read_csv('flood_vulnerable_locations.csv')\n",
    "\n",
    "# Ensure the weather_data and flood_vulnerable_loc have the same lat and long format\n",
    "weather_data['latitude'] = weather_data['latitude'].round(5)  # Correct column name\n",
    "weather_data['longitude'] = weather_data['longitude'].round(5)  # Correct column name\n",
    "\n",
    "# Check the first few rows of both datasets to ensure proper formatting\n",
    "print(\"Weather Data Columns and First Few Rows:\")\n",
    "print(weather_data.head())\n",
    "print(\"\\nFlood Vulnerable Locations Columns and First Few Rows:\")\n",
    "print(flood_vulnerable_loc.head())\n",
    "\n",
    "# Merge the weather data with the flood_vulnerable_loc to get location names\n",
    "merged_data = pd.merge(weather_data, flood_vulnerable_loc, on=['latitude', 'longitude'], how='left')\n",
    "\n",
    "# Display the merged data to see the location names along with the flood predictions\n",
    "print(\"\\nMerged Data with Location Names and Flood Predictions:\")\n",
    "print(merged_data[['longitude', 'latitude', 'LocationName', 'WARD_NAME', 'predicted_flood']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flood prediction results saved to predicted_flood_locations.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the merged data with flood predictions to a CSV file\n",
    "merged_data.to_csv('predicted_flood_locations.csv', index=False)\n",
    "print(\"Flood prediction results saved to predicted_flood_locations.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
